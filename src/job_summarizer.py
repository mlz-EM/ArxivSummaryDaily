"""
å·¥ä½œæ€»ç»“æ¨¡å— - ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹APIç”Ÿæˆå·¥ä½œæœºä¼šæ¦‚å†µ
"""
import os
import json
from typing import List, Dict, Any, Optional
from pathlib import Path
import requests
import time
from datetime import datetime
import pytz
from config.settings import LLM_CONFIG, QUERY

class ModelClient:
    """è¯­è¨€æ¨¡å‹APIå®¢æˆ·ç«¯"""
    
    def __init__(self, api_key: str, model: Optional[str] = None):
        self.api_key = api_key
        self.model = model or LLM_CONFIG['model']
        self.api_url = f"{LLM_CONFIG['api_url']}/{self.model}:generateContent"
        self.timeout = LLM_CONFIG.get('timeout', 30)
        
    def _create_headers(self) -> Dict[str, str]:
        """åˆ›å»ºè¯·æ±‚å¤´"""
        return {
            "Content-Type": "application/json"
        }
    
    def _create_request_body(
        self, 
        messages: List[Dict[str, str]],
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None
    ) -> Dict[str, Any]:
        """åˆ›å»ºè¯·æ±‚ä½“"""
        # å°†æœ€åä¸€æ¡æ¶ˆæ¯ä½œä¸ºæç¤ºè¯
        prompt = messages[-1]["content"]
        
        return {
            "contents": [{
                "parts": [{
                    "text": prompt
                }]
            }],
            "generationConfig": {
                "temperature": temperature or LLM_CONFIG['temperature'],
                "maxOutputTokens": max_tokens or LLM_CONFIG['max_output_tokens'],
                "topP": LLM_CONFIG['top_p'],
                "topK": LLM_CONFIG['top_k']
            }
        }
    
    def chat_completion(
        self,
        messages: List[Dict[str, str]],
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None
    ) -> Dict[str, Any]:
        """åˆ›å»ºèŠå¤©å®Œæˆ"""
        headers = self._create_headers()
        data = self._create_request_body(messages, temperature, max_tokens)
        
        for attempt in range(LLM_CONFIG['retry_count']):
            try:
                response = requests.post(
                    f"{self.api_url}?key={self.api_key}",
                    headers=headers,
                    json=data,
                    timeout=self.timeout
                )
                
                if response.status_code != 200:
                    raise Exception(f"API è°ƒç”¨å¤±è´¥: {response.text}")
                    
                result = response.json()
                
                return {
                    "choices": [{
                        "message": {
                            "role": "assistant",
                            "content": result["candidates"][0]["content"]["parts"][0]["text"]
                        },
                        "finish_reason": "stop"
                    }],
                    "usage": {
                        "prompt_tokens": 0,
                        "completion_tokens": 0,
                        "total_tokens": 0
                    }
                }
            except requests.Timeout:
                print(f"è¯·æ±‚è¶…æ—¶ï¼ˆ{self.timeout}ç§’ï¼‰ï¼Œæ­£åœ¨é‡è¯•...")
                if attempt == LLM_CONFIG['retry_count'] - 1:
                    raise TimeoutError(f"APIè°ƒç”¨åœ¨{self.timeout}ç§’å†…æœªå“åº”ï¼Œå·²é‡è¯•{LLM_CONFIG['retry_count']}æ¬¡")
                time.sleep(LLM_CONFIG['retry_delay'] * (2 ** attempt))
            except Exception as e:
                if attempt == LLM_CONFIG['retry_count'] - 1:
                    raise
                time.sleep(LLM_CONFIG['retry_delay'] * (2 ** attempt))

class JobSummarizer:
    def __init__(self, api_key: str, model: Optional[str] = None):
        self.client = ModelClient(api_key, model)
        self.max_papers_per_batch = 20

    def _generate_batch_summaries(self, jobs: List[Dict[str, Any]], start_index: int) -> str:
        """ä¸ºä¸€æ‰¹è®ºæ–‡ç”Ÿæˆæ€»ç»“"""
        batch_prompt = ""
        for i, job in enumerate(jobs, start=start_index):
            batch_prompt += f"""
job {i}ï¼š
title: {job['title']}
school: {job['company']}
location: {job['location']}
posted: {job['date_posted']}
description: {job['description']}
job_url: {job['job_url']}
"""
        
        final_prompt = f"""æˆ‘æ˜¯ä¸€åææ–™å·¥ç¨‹ç³»çš„åšå£«æ¯•ä¸šç”Ÿï¼Œæˆ‘çš„ç ”ç©¶é¢†åŸŸæ˜¯ç”¨ç”µå­æ˜¾å¾®é•œåœ¨å¾®è§‚å°ºåº¦ä¸Šè¿›è¡Œææ–™è¡¨å¾å¹¶å»ºç«‹å…¶ç»“æ„ä¸æ€§èƒ½ä¹‹é—´çš„è”ç³»ã€‚ç›®å‰æˆ‘åœ¨å¯»æ‰¾åŒ—ç¾tenure trackedçš„æ•™èŒã€‚æˆ‘å°†æä¾›{len(jobs)}ä¸ªæ½œåœ¨å·¥ä½œæœºä¼šï¼Œè¯·æ ¹æ®descriptionæˆ–è€…job_urlçš„å†…å®¹åˆ†åˆ«ç”Ÿæˆmarkdownè¯­è¨€æ ¼å¼çš„æ€»ç»“ã€‚å¯¹æ¯ä»½å·¥ä½œï¼š
1. åˆ é™¤é¢†åŸŸå®Œå…¨ä¸ç›¸å…³çš„å·¥ä½œï¼Œä¾‹å¦‚æ–‡ç§‘ç±»å·¥ä½œï¼ŒåŒ»å­¦é™¢å·¥ä½œï¼Œä»¥åŠç®¡ç†ç±»å·¥ä½œã€‚
2. åˆ é™¤ä¸æ˜¯tenure trackedçš„å·¥ä½œï¼Œä¾‹å¦‚teaching facultyæˆ–è€…adjunct professorã€‚
3. æŸ¥è¯¢å­¦æ ¡æ˜¯å¦ä¸ºR1ï¼Œå¦‚æœä¸æ˜¯è¯·åˆ é™¤è¯¥å·¥ä½œã€‚
4. å¦‚æœå…¨éƒ¨å·¥ä½œéƒ½ä¸æ»¡è¶³ä»¥ä¸Šæ¡ä»¶ï¼Œä¹Ÿè¯·ä¿ç•™ä¸€ä»½æœ€ç›¸å…³çš„å·¥ä½œå¹¶ç”Ÿæˆæ€»ç»“ã€‚
5. æ ¹æ®ä¸æˆ‘èƒŒæ™¯çš„å¯¹å·¥ä½œç›¸å…³ç¨‹åº¦å¯¹ç­›é€‰åçš„å·¥ä½œè¿›è¡Œæ‰“åˆ† ä»ä¸€é¢—åˆ°ä¸‰é¢—ğŸŒŸ
6. åœ¨å·¥ä½œæè¿°ä¸­æå–ä¸€å¥è¯çš„å…³é”®è¯è¿›è¡Œæ€»ç»“ï¼Œæœ€å¥½æ˜¯å·¥ä½œéœ€è¦çš„å…·ä½“æ–¹å‘æˆ–è€…department
è¯·ç”¨è‹±æ–‡å›ç­”ï¼Œä¿æŒåŸæœ‰æ ¼å¼ï¼Œå¯¹æ¯ä»½å·¥ä½œçš„å›ç­”ååŠ å…¥markdownæ ¼å¼çš„"---"åˆ†éš”ç¬¦ã€‚
ç¡®ä¿æ¯ä»½å·¥ä½œä¿¡æ¯ä¸æä¾›çš„å†…å®¹ä¿æŒä¸€è‡´ã€‚
ä½ çš„è¾“å‡ºç¯å¢ƒåŒæ—¶æ”¯æŒmarkdownå’ŒLaTeXè¯­æ³•æ¸²æŸ“
è¾“å‡ºæ ¼å¼ä¸ºï¼š

**[title](job_url)** ğŸŒŸğŸŒŸ
- **Location**: school at location
- **Date**: YYYY-MM-DD
- **Description**: summary
---
**[title](job_url)** ğŸŒŸ
- **Location**: school at location
- **Date**: YYYY-MM-DD
- **Description**: summary
---
......
---
**[title](job_url)** ğŸŒŸğŸŒŸğŸŒŸ
- **Location**: school at location
- **Date**: YYYY-MM-DD
- **Description**: summary
---

è¯·æ³¨æ„ï¼Œä»¥ä¸Šæ˜¯å¯¹æ¯ä»½å·¥ä½œçš„æ€»ç»“æ ¼å¼ç¤ºä¾‹ã€‚è¯·ç¡®ä¿è¾“å‡ºæ ¼å¼ä¸ç¤ºä¾‹ä¸€è‡´ã€‚ä¸è¦æ·»åŠ ä»»ä½•é¢å¤–ä¿¡æ¯ï¼Œåªç”Ÿæˆè§„å®šæ ¼å¼çš„æ€»ç»“å†…å®¹å³å¯ã€‚è¯·ä¸€å®šç¡®ä¿æ ¼å¼äº‰å–ã€‚

ä»¥ä¸‹æ˜¯ä¸€ä¸ªç¤ºä¾‹ï¼š

---
**[Assistant Professor in Materials Sciecne Department](http://linkedin.com/job)** ğŸŒŸğŸŒŸğŸŒŸ
- **Location**: Harvard Univeersity at Boston, USA
- **Date**: 2025-01-11
- **Description**: Department of Materials Sciecne is looking for TT prof to work on the characterization of energy-related materials.
---

è¯·æ ¹æ®ä»¥ä¸‹å·¥ä½œä¿¡æ¯ç”Ÿæˆæ€»ç»“ï¼š
{batch_prompt}"""

        try:
            response = self.client.chat_completion([{
                "role": "user",
                "content": final_prompt
            }])
            nr = response["choices"][0]["message"]["content"].strip().count('**Description**:')
            print(f"æ‰¹é‡å¤„ç†å®Œæˆï¼Œç”Ÿæˆ{nr}ä»½æ€»ç»“...")
            return response["choices"][0]["message"]["content"].strip()
        except Exception as e:
            # å¦‚æœæ‰¹å¤„ç†å¤±è´¥ï¼Œç”Ÿæˆé”™è¯¯ä¿¡æ¯
            error_summaries = []
            for i, job in enumerate(jobs, start=start_index):
                error_summaries.append(f"""
job {i}ï¼š
title: {job['title']}
school: {job['company']}
location: {job['location']}
posted: {job['date_posted']}
description: {job['description']}
job_url: {job['job_url']}
summary: [ç”Ÿæˆå¤±è´¥: {str(e)}]
---""")
            return "\n".join(error_summaries)

    def _process_batch(self, jobs: List[Dict[str, Any]], start_index: int) -> str:
        """å¤„ç†ä¸€æ‰¹å·¥ä½œ"""
        print(f"æ­£åœ¨æ‰¹é‡å¤„ç† {len(jobs)} ä»½å·¥ä½œ...")
        summaries = self._generate_batch_summaries(jobs, start_index)
        time.sleep(2)  # åœ¨æ‰¹æ¬¡ä¹‹é—´æ·»åŠ çŸ­æš‚å»¶è¿Ÿ
        return summaries

    def _generate_batch_summary(self, jobs: List[Dict[str, Any]]) -> str:
        """æ‰¹é‡ç”Ÿæˆæ‰€æœ‰å·¥ä½œçš„æ€»ç»“"""
        all_summaries = []
        total_jobs = len(jobs)
        
        for i in range(0, total_jobs, self.max_papers_per_batch):
            batch = jobs[i:i + self.max_papers_per_batch]
            print(f"\næ­£åœ¨å¤„ç†ç¬¬ {i + 1} åˆ° {min(i + self.max_papers_per_batch, total_jobs)} ä»½å·¥ä½œ...")
            batch_summary = self._process_batch(batch, i + 1)
            all_summaries.append(batch_summary)
            
            if i + self.max_papers_per_batch < total_jobs:
                print("æ‰¹æ¬¡å¤„ç†å®Œæˆï¼Œç­‰å¾…3ç§’åç»§ç»­...")
                time.sleep(3)  # æ‰¹æ¬¡ä¹‹é—´çš„å†·å´æ—¶é—´
        
        return "\n".join(all_summaries)

    def summarize_jobs(self, jobs: List[Dict[str, Any]], output_file: str) -> bool:
        """
        æ‰¹é‡å¤„ç†æ‰€æœ‰è®ºæ–‡å¹¶åˆ›å»ºMarkdownæŠ¥å‘Š
        
        Args:
            papers: è®ºæ–‡åˆ—è¡¨
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            
        Returns:
            bool: æ‘˜è¦ç”Ÿæˆæ˜¯å¦çœŸæ­£æˆåŠŸã€‚å¦‚æœç”Ÿæˆçš„æ‘˜è¦åŒ…å«é”™è¯¯ä¿¡æ¯åˆ™è¿”å›False
        """
        api_success = True  # æ ‡è®°APIè°ƒç”¨æ˜¯å¦æˆåŠŸ
        
        try:
            # ç”Ÿæˆæ€»ç»“å†…å®¹
            print(f"å¼€å§‹ç”Ÿæˆå·¥ä½œæ€»ç»“ï¼Œå…± {len(jobs)} ä»½...")
            summaries = self._generate_batch_summary(jobs)
            
            # æ£€æŸ¥ç”Ÿæˆçš„æ‘˜è¦æ˜¯å¦åŒ…å«é”™è¯¯ä¿¡æ¯
            if "[ç”Ÿæˆå¤±è´¥:" in summaries:
                api_success = False
                print("è­¦å‘Š: æ‘˜è¦ç”Ÿæˆè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼Œç»“æœå¯èƒ½ä¸å®Œæ•´")
            
            # è½¬æ¢ä¸ºmarkdownæ ¼å¼
            markdown_content = self._generate_markdown(jobs, summaries)
            
            # ä¿å­˜ä¸ºmarkdownæ–‡ä»¶
            output_md = output_file.replace('.pdf', '.md')
            with open(output_md, 'w', encoding='utf-8') as f:
                f.write(markdown_content)
            print(f"Markdownæ–‡ä»¶å·²ä¿å­˜ï¼š{output_md}")
            
            return api_success
            
        except Exception as e:
            # å¦‚æœç”Ÿæˆæ€»ç»“å¤±è´¥ï¼Œä¿å­˜åŸºæœ¬ä¿¡æ¯ä¸ºmarkdownæ ¼å¼
            beijing_time = datetime.now(pytz.timezone('America/New_York')).strftime('%Y-%m-%d %H:%M:%S')
            error_content = f"""# Arxivè®ºæ–‡æ€»ç»“æŠ¥å‘Š

ç”Ÿæˆæ—¶é—´ï¼š{beijing_time}

**ç”Ÿæˆæ€»ç»“æ—¶å‘ç”Ÿé”™è¯¯ï¼Œä»¥ä¸‹æ˜¯è®ºæ–‡åŸºæœ¬ä¿¡æ¯ï¼š**

"""
            for i, job in enumerate(jobs, 1):
                error_content += f"""
job {i}ï¼š
title: {job['title']}
school: {job['company']}
location: {job['location']}
posted: {job['date_posted']}
description: {job['description']}
job_url: {job['job_url']}
summary: [ç”Ÿæˆå¤±è´¥: {str(e)}]
"""
            
            # ä¿å­˜é”™è¯¯ä¿¡æ¯ä¸ºmarkdownæ–‡ä»¶
            error_md = output_file.replace('.pdf', '_error.md')
            with open(error_md, 'w', encoding='utf-8') as f:
                f.write(error_content)
            print(f"å‘ç”Ÿé”™è¯¯ï¼Œå·²ä¿å­˜åŸºæœ¬ä¿¡æ¯åˆ°ï¼š{error_md}")
            
            return False  # å‘ç”Ÿå¼‚å¸¸ï¼Œæ‘˜è¦ç”Ÿæˆè‚¯å®šå¤±è´¥

    def _generate_markdown(self, jobs: List[Dict[str, Any]], summaries: str) -> str:
        """ç”Ÿæˆmarkdownæ ¼å¼çš„æŠ¥å‘Š"""
        beijing_time = datetime.now(pytz.timezone('America/New_York')).strftime('%Y-%m-%d %H:%M:%S')
        
        markdown_content = f"""
# Basic Info
- This report was automatically generated by **{self.client.model}** at **{beijing_time}**.  
- It includes the recent tenure tracked position in engineering related field from Google, LinkedIn and Indeed.  
- This page is powered by [ArxivSummaryDaily](https://github.com/dong-zehao/ArxivSummaryDaily) and [JobSpy](https://github.com/speedyapply/JobSpy).
---
{summaries}
"""
        return markdown_content
